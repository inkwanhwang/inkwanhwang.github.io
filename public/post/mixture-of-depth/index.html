<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: August 11, 2024 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.2.0" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Inkwan Hwang" />

  
  
  
    
  
  <meta name="description" content="Create a personal knowledge base and share your knowledge with your peers." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/post/mixture-of-depth/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/blue.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu3247630877640252165.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu4166356570829923896.png" />

  <link rel="canonical" href="http://localhost:1313/post/mixture-of-depth/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@GetResearchDev" />
    <meta property="twitter:creator" content="@GetResearchDev" />
  
  <meta property="og:site_name" content="Inkwan Hwang" />
  <meta property="og:url" content="http://localhost:1313/post/mixture-of-depth/" />
  <meta property="og:title" content="ü§ñ Accelerating Transformers via Conditional Computation - As Aspect of Mixture-of-Depths | Inkwan Hwang" />
  <meta property="og:description" content="Create a personal knowledge base and share your knowledge with your peers." /><meta property="og:image" content="http://localhost:1313/post/mixture-of-depth/featured.jpg" />
    <meta property="twitter:image" content="http://localhost:1313/post/mixture-of-depth/featured.jpg" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2023-10-26T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2023-10-26T00:00:00&#43;00:00">
  

  



  


  <title>ü§ñ Accelerating Transformers via Conditional Computation - As Aspect of Mixture-of-Depths | Inkwan Hwang</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
    
  
  
  
  
  














  
  
  <link type="text/css" rel="stylesheet" href="/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css" integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr&#43;QR0SQDNfgglgtcM=" />
  
  
  <script defer src="/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js" integrity="sha256-3ISyluw&#43;iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script>
  
  
  
  
  <script defer src="/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js" integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  






  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Inkwan Hwang">
        Inkwan Hwang
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >About</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/background/"
        >Background</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/publication/"
        >Publication</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/projects/"
        >Projects</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/awards/"
        >Honors &amp; Awards</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/teaching/"
        >Mentoring</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/blog/"
        >Blog</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>



        
      
    </div>
    <div class="page-body  my-10">
      





<div class="mx-auto flex max-w-screen-xl">
  



<aside class="hb-sidebar-container max-lg:[transform:translate3d(0,-100%,0)] lg:hidden xl:block">
  
  <div class="px-4 pt-4 lg:hidden">
    
    
  </div>
  <div class="hb-scrollbar lg:h-[calc(100vh-var(--navbar-height))]">
    <ul class="flex flex-col gap-1 lg:hidden">
      
      
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/"
    
  >Recent &amp; Upcoming Talks
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/example/"
    
  >Example Talk
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/teaching/"
    
  >Teaching
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/awards/"
    
  >Awards
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/background/"
    
  >Background
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/blog/"
    
  >Blog
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/projects/"
    
  >Projects
    </a></li>
        <li class="open"><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/"
    
  >Blog
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col open"><a
    class="hb-sidebar-custom-link
      sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900"
    href="/post/mixture-of-depth/"
    
  >ü§ñ Accelerating Transformers via Conditional Computation - As Aspect of Mixture-of-Depths
    </a>
  
    <ul class="hb-sidebar-mobile-toc"><li>
              <a
                href="#introduction"
                class="hb-docs-link"
              >&lt;strong&gt;Introduction&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#understanding-the-problem-uniform-computation-in-transformers"
                class="hb-docs-link"
              >&lt;strong&gt;Understanding the problem: Uniform computation in Transformers&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#conditional-computation-for-transformers"
                class="hb-docs-link"
              >&lt;strong&gt;Conditional computation for Transformers&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#overview-to-mixture-of-depths-mod"
                class="hb-docs-link"
              >&lt;strong&gt;Overview to Mixture-of-Depths (MoD)&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#routing-schemes"
                class="hb-docs-link"
              >&lt;strong&gt;Routing schemes&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#routing"
                class="hb-docs-link"
              >&lt;strong&gt;Routing&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#implementation"
                class="hb-docs-link"
              >&lt;strong&gt;Implementation&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#open-source-mod-not-official"
                class="hb-docs-link"
              >&lt;strong&gt;Open source MoD&lt;/strong&gt; (not official)</a>
            </li>
          <li>
              <a
                href="#results"
                class="hb-docs-link"
              >&lt;strong&gt;Results&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#conclusion-and-discussion"
                class="hb-docs-link"
              >&lt;strong&gt;Conclusion and discussion&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#references"
                class="hb-docs-link"
              >&lt;strong&gt;References&lt;/strong&gt;</a>
            </li>
          </ul>
  
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/summer-break/"
    
  >üèñÔ∏è Summer Break - Chance to See the Wider World
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/"
    
  >Projects
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/pandas/"
    
  >Pandas
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/pytorch/"
    
  >PyTorch
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/scikit/"
    
  >scikit-learn
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/"
    
  >Publications
    </a></li>
    </ul>

    <div class="max-xl:hidden h-0 w-64 shrink-0"></div></div>

</aside>
  

<nav class="hb-toc order-last hidden w-64 shrink-0 xl:block print:hidden px-4" aria-label="table of contents">
  











  <div class="hb-scrollbar text-sm [hyphens:auto] sticky top-16 overflow-y-auto pr-4 pt-6 max-h-[calc(100vh-var(--navbar-height)-env(safe-area-inset-bottom))] -mr-4 rtl:-ml-4"><p class="mb-4 font-semibold tracking-tight">On this page</p><ul>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#introduction">Introduction</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#understanding-the-problem-uniform-computation-in-transformers">Understanding the problem: Uniform computation in Transformers</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#conditional-computation-for-transformers">Conditional computation for Transformers</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#overview-to-mixture-of-depths-mod">Overview to Mixture-of-Depths (MoD)</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#routing-schemes">Routing schemes</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#token-choice-routing">Token-choice routing</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#expert-choice-routing">Expert-choice routing</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#expert-choice-mod">Expert-choice MoD</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#routing">Routing</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#implementation">Implementation</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#capacity">Capacity</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#autoregressively-sampling">Autoregressively sampling</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#open-source-mod-not-official">Open source MoD (not official)</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#results">Results</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#hyperparameter-tuning">Hyperparameter tuning</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#isoflop-analysis">isoFLOP analysis</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#auto-regressive-evaluation">Auto-regressive evaluation</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#mixture-of-depths-and-experts-mode">Mixture-of-Depths-and-Experts (MoDE)</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#conclusion-and-discussion">Conclusion and discussion</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#references">References</a>
      </li></ul>

  
  



    












  </div>
  </nav>


  <article class="w-full break-words flex min-h-[calc(100vh-var(--navbar-height))] min-w-0 justify-center pb-8 pr-[calc(env(safe-area-inset-right)-1.5rem)]">
    <main class="w-full min-w-0 max-w-6xl px-6 pt-4 md:px-12">

      

      <h1 class="mt-2 text-4xl font-bold tracking-tight text-slate-900 dark:text-slate-100">ü§ñ Accelerating Transformers via Conditional Computation - As Aspect of Mixture-of-Depths</h1>

      <div class="mt-4 mb-16">
      <div class="text-gray-500 dark:text-gray-300 text-sm flex items-center flex-wrap gap-y-2"><span class="mr-1">Oct 26, 2023</span><span class="mx-1">¬∑</span>
        
          
          
          <div class="group inline-flex items-center text-current gap-x-1.5 mx-1">
            
            
            <img src="/author/inkwan-hwang/avatar_hu5229914575614502685.webp" alt="Inkwan Hwang" class="inline-block h-4 w-4 rounded-full border border-current" loading="lazy" />
            
            <div >Inkwan Hwang</div>
          </div>
          
        
          
          <span class="mr-1">,</span>
          <div class="group inline-flex items-center text-current gap-x-1.5 mx-1">
            
            <div >Minjae Park</div>
          </div>
          
        

        
        <span class="mx-1">¬∑</span>
        <span class="mx-1">
          10 min read
        </span>
        
        </div>

        <div class="mt-3">
          





        </div>
      </div>



      
      
      
      
      

      
      
      
      
      
      
      
      
      
      
      <div class="article-header article-container featured-image-wrapper mt-4 mb-16" style="max-width: 720px; max-height: 720px;">
        <div style="position: relative">
          <img src="/post/mixture-of-depth/featured_hu6515972531203818294.webp" width="720" height="720" alt="" class="featured-image">
          <span class="article-header-caption">Image credit:</span>
        </div>
      </div>
      

      
      

      <div class="prose prose-slate lg:prose-xl dark:prose-invert">
        <p>This post is cross-posted at <a href="https://effml-postech.github.io/docs/spring24/16_/" target="_blank" rel="noopener">EffL POSTECH</a></p>
<h2 id="introduction"><strong>Introduction</strong></h2>
<p>‚ÄúChoice and concentration‚Äù is an effective strategy for achieving success in problems. Sometimes, it is not necessary to put the same amount of effort and time into all problems. Expending energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.</p>
<p>In this post, We will explain conditional computation strategies for Transformers, focusing on a technology announced this year called <strong>Mixture-of-Depths.</strong></p>
<p>paper: <U><a href="https://arxiv.org/abs/2404.02258" target="_blank"> Mixture-of-Depths: Dynamically allocating compute in transformer-based language models </a></U></p>
<p>Let&rsquo;s dive in!</p>
<h2 id="understanding-the-problem-uniform-computation-in-transformers"><strong>Understanding the problem: Uniform computation in Transformers</strong></h2>
<p>These days, most language models are based on Transformers, and we stack these blocks to make big models. When given an input sequence, tokens pass through these blocks to predict the next token. The problem is that the models spread computations uniformly across input sequences. Transformers use the same amount of computation for essential tokens as for non-essential ones. For instance, predicting a token within a sentence is cheaper than predicting the first token of the next sentence. Researchers want to address this issue by making Transformers focus on important tokens by allocating unimportant tokens with fewer computing resources.</p>
<h2 id="conditional-computation-for-transformers"><strong>Conditional computation for Transformers</strong></h2>
<ul>
<li>
<p><strong>Early exiting</strong></p>
<p align="center">
  <img src=./Early_Exiting.png> 
</p>
Instead of passing through all layers, the model can stop early if it is confident enough about its prediction. This saves computation time and resources. Large pre-trained models like BERT can use early exiting to maintain performance while reducing computing resources.
</li>
<li>
<p><strong>CoLT5</strong></p>
<p align="center">
  <img src=./colt1.png width = "50%" height = "50%">
</p>
CoLT5 is an architecture allowing unnecessary tokens pass through light attention and light MLP. Light attention refers to a local attention layer that just calculates attention value between a few nearby tokens. Conversely, heavy Attention refers to a global attention layer that calculates some chosen token (chosen by router) and calculates attention values with all input tokens. It uses top-k routing mechanism that performs well (will be discussed in a later section).
<p align="center">
  <img src=./colt2.png width = "40%" height = "40%">
</p>
The figure above is the attention map in CoLT5. Light-colored ones indicate light attention(local attention) and bold ones indicate heavy attention. The model chooses 1/16 of query tokens and 1/8 of key tokens for heavy attention calculation.
</li>
<li>
<p><strong>Mixture of Experts (MoE)</strong></p>
<p align="center">
  <img src=./moe1.png>
</p>
MoE is a model consisting of parallel expert models which is fitted to certain domains. Token-level routing decisions are made across the network depths. Routing decision of the model determines which expert it will be sent to.
</li>
</ul>
<h2 id="overview-to-mixture-of-depths-mod"><strong>Overview to Mixture-of-Depths (MoD)</strong></h2>
<p>Our goal is to reduce the overall FLOPs by focusing on essential tokens and relatively fewer non-essential tokens. The router is responsible for determining the path each token should take. A trained router evaluates whether the token is necessary. If the token is deemed essential, it passes through self-attention and the subsequent MLP (requiring FLOPs). Otherwise, it bypasses these stages via a residual connection (saving FLOPs).</p>
<p align="center">
    <img src=./Mixture-of-Depths.png> 
</p>
 The above image depicts the path of the model with an input sequence length of 64. The purple color shows the computation performed by that layer and the orange color shows the path taken by the residual connection.
<h2 id="routing-schemes"><strong>Routing schemes</strong></h2>
<p>Routing implementation is the most crucial part of MoD. Authors compare three routing schemes, demonstrating that MoD is the most efficient approach.</p>
<p align="center">
    <img src=./Routing_Schemes.png> 
</p>
<h3 id="token-choice-routing"><strong>Token-choice routing</strong></h3>
<p>Token-choice routing is a method where each token selects the path it will follow. The router produces probability distributions for each token across the computational paths. Based on this distribution, each token chooses its preferred path at each layer.</p>
<p>In this scheme, tokens have the flexibility to select their path, allowing for dynamic processing. However, this can lead to path-balancing issues as all tokens might prefer on the same path. It causes potential overloads on specific paths. To mitigate it, auxiliary loss is used to ensure that most tokens do not prefer a single path.</p>
<h3 id="expert-choice-routing"><strong>Expert-choice routing</strong></h3>
<p>Expert-choice routing is the reverse version of token-choice routing. Similar to token-choice routing, the router produces a probability distribution for each token. In this scheme, instead of tokens selecting their paths, each path selects the top-$k$
 tokens based on the experts&rsquo; preferences.</p>
<p>Using this method ensures that each path receives k tokens, maintaining balance among the paths. However, some tokens may not be selected because there might be common tokens that multiple paths prefer.</p>
<h3 id="expert-choice-mod"><strong>Expert-choice MoD</strong></h3>
<p>This method is advantageous as it reduces the overall FLOPs in the model&rsquo;s forward pass. When k is smaller than the input sequence length, some tokens do not need to undergo self-attention and MLP computations. For the left and middle approaches in the figure, selecting the top-k tokens may result in increased FLOPs since multiple experts need to perform computations.</p>
<p>For the following reasons, the authors decided to use expert-choice routing and utilize only single path:</p>
<ul>
<li><strong>Efficiency of computation</strong>
There is no need for an auxiliary balancing loss.</li>
<li><strong>Simplicity of implementation</strong>
Tokens can be chosen with the highest output value of router in order.</li>
<li><strong>Clear criteria</strong>
Top-k strategy can guarantee that the most important token is calculated since the top-$k$
 tokens are independent of the magnitude of router weights. Since tokens are divided into two sets, one passing through self-attention and MLP, and the other passing through residual connections, a strategy is needed to partition tokens into these two sets.</li>
</ul>
<h2 id="routing"><strong>Routing</strong></h2>
<ul>
<li>$l$
 is a given layer.</li>
<li>$S$
 is a sequence length.</li>
<li>$\beta=1-C/S$
 is an user-defined capacity per batch element.</li>
<li>$f$
 comprises self-attention and subsequent MLP.</li>
</ul>

$$
x^{l+1}_i=\begin{cases}r^{l}_i f_i(\tilde{X}^l)+x^{l}_i, &    \text{if } r^{l}_i >  P_\beta(R^l)\\x^{l}_i, & \text{if }r^{l}_i <  P_\beta(R^l)\end{cases}
$$


<p>Find the $\beta$
-th percentile ($P_\beta(R^l)$
) of the set of router weights $R^l$
. If the router weight $r^l$
 is greater than $P_\beta(R^l)$
, perform self-attention and subsequent MLP computations. If it is less than $P_\beta(R^l)$
, pass through the token residual connection.</p>
<h2 id="implementation"><strong>Implementation</strong></h2>
<h3 id="capacity"><strong>Capacity</strong></h3>
<p>In this paper, capacity-based routing is employed. Token <em>capacity</em> is the total proportion of tokens composing the input for a given operation. For instance, if the input sequence length is 100 and the capacity is 20%, each layer operates on the top-20 tokens determined by router weights.</p>
<p>By lowering the capacity of the computations, a smaller compute budget can be utilized per forward pass compared to the vanilla Transformers. In MoD, capacity is utilized as a hyperparameter to determine the proportion of tokens processed per layer operation.</p>
<h3 id="autoregressively-sampling"><strong>Autoregressively sampling</strong></h3>
<p>We&rsquo;re looking to implement expert-choice routing, but there is one distinct problem: top-k operations rely on future tokens! Our goal is for each token to determine if it belongs to the top-k using routers. To do this, every token needs access to the router weights of future tokens. Unfortunately, we cannot predict the future router weights and cannot employ autoregressive sampling. To solve this problem, the authors propose two methods.</p>
<ul>
<li>
<p><strong>Simple auxiliary loss</strong></p>
<p align="center">
  <img src=./Routing_Analysis.png width = "30%" height = "30%"> 
</p>
Designing an additional binary cross-entropy loss function at the router's output can resolve this issue. By incorporating this, the value of tokens in the top-k is guided to be greater than 0.5, while the value of tokens not in the top-k is guided to be less than 0.5. As token passes through the router, they are categorized into top-k set if their value exceeds 0.5. Then it passes through self-attention and subsequent MLP. Conversely, tokens with values below 0.5 pass through the residual connection. Integrating such a function impacts the primary language modeling objective by approximately 0.2-0.3%. We believe this likely refers to the extent to which performance and inference time are affected.
</li>
<li>
<p><strong>Small auxiliary MLP predictor</strong></p>
<p>The second method does not affect the primary language modeling objective at all. The authors design a new MLP layer that functions as a binary classifier to determine whether a token is in top-k during the training process. This classifier is trained to make these determinations, and it is used in real time during the autoregressive sampling process.</p>
</li>
</ul>
<p>These methods, in autoregressive situation, could predict whether given token is important or not in real-time. They provide empirical results that auxiliary tasks achieved 99% accuracy.</p>
<h2 id="open-source-mod-not-official"><strong>Open source MoD</strong> (not official)</h2>
<p>The following is an implementation of MoD that supports various LMs such as Mixtral, LLama3, and BLOOM. It implements MoD using PyTorch and Hugging Face Transformers library.</p>
<p>LINK: <a href="https://github.com/astramind-ai/Mixture-of-depths" target="_blank" rel="noopener">https://github.com/astramind-ai/Mixture-of-depths</a></p>
<h2 id="results"><strong>Results</strong></h2>
<h3 id="hyperparameter-tuning"><strong>Hyperparameter tuning</strong></h3>
<p align="center">
    <img src=./result2.png>
</p>
<p>The authors first trained the model with a limited FLOPs budget (6e18) to determine the optimal hyperparameters. Through training the MoD Transformer with routing blocks and self-attention blocks arranged alternately, they found the optimal parameters. The two top-middle graphs show the actual training loss graphs for the points plotted in the left graph. Among them, MoD with 12.5% capacity generally results in lower loss values than the baseline.</p>
<ul>
<li><strong>Computation efficiency</strong>: In the right graph, the points #1, #3 and  #2, #4 pairs are MoD models of the same parameter size. Not only does it have a lower loss value, but it also runs approximately 66% faster than the baseline.</li>
</ul>
<h3 id="isoflop-analysis"><strong>isoFLOP analysis</strong></h3>
<p align="center">
    <img src=./result1.png>
</p>
<p>In this figure, the training FLOPs budget is limited to 6e18, 2e19, and 1e20 comparing isoFLOP baseline and 12.5% capacity MoD.</p>
<ul>
<li><strong>Total loss</strong>: The graph in the top-left corner shows that the isoFLOP baseline has a slightly better loss when the number of parameters is small (Note that there is a crossing point!).</li>
<li><strong>Normalized loss</strong>: When the x-axis is converted from parameters to FLOPs per FFW (Forward Pass) as shown in the top-right graph, MoD is better than the baseline in all cases.</li>
</ul>
<h3 id="auto-regressive-evaluation"><strong>Auto-regressive evaluation</strong></h3>
<p align="center">
    <img src=./result3.png>
</p>
<p>MoD variants were evaluated during auto-regressive sampling. Each model was tested on data comprising 256,000 sequences.</p>
<ul>
<li><strong>Predictor accuracy</strong>: Using predictor-based methods is cheaper than top-k but not more accurate. In the left graph, the performance of the predictor strategy is almost indistinguishable from the top-k strategy. Authors attribute this to the ease of learning this prediction problem.</li>
</ul>
<h3 id="mixture-of-depths-and-experts-mode"><strong>Mixture-of-Depths-and-Experts (MoDE)</strong></h3>
<p align="center">
    <img src=./result4.png>
</p>
This figure shows the performance of MoDE and its two proposed structures. The top-left graph demonstrates that the performance of MoDE is better than both the Baseline and MoE. The right side explains the structures of Staged MoDE and Integrated MoDE.
<ul>
<li><strong>Staged MoDE</strong>: Two routers are deployed to first for determine the depth (MoD) and second for the expert (MoE).</li>
<li><strong>Integrated MoDE</strong>: The MoD router and MoE router are integrated into one single Router that can simultaneously decide whether to select an expert or the residual path (depth).</li>
</ul>
<p>The paper mentions that the former is computationally efficient as it can skip self-attention operations through the MoD router, and the latter has better performance as the router mechanism is unified and self-attention operations are always performed.</p>
<h2 id="conclusion-and-discussion"><strong>Conclusion and discussion</strong></h2>
<p>This paper insists that using MoD with a capacity 12.5% is better than the baseline transformer model.</p>
<p>However, there are some unresolved limitations not discussed in the paper.</p>
<ul>
<li>
<p><strong>Only loss values</strong>: We believe this approach only indicates if parameters converge to the training dataset, not the model&rsquo;s performance. To ensure MoD&rsquo;s superiority over the baseline model, additional evaluation methods such as perplexity (WikiText-2, Lambada) and specific tasks (BoolQ, Hellaswag, etc.) should be included.</p>
</li>
<li>
<p><strong>More experiments are needed</strong>: The paper only compares loss values for 12.5% and 50% capacity. They also applied MoD in one of two layers, but there are no comments on why applying this method. Further studies about using one of three or four should be done.</p>
</li>
<li>
<p><strong>More baselines are needed</strong>: Further studies should provide validation of MoD method by comparing other methods like COLT5 or MoE and proof of optimal hyperparameters.</p>
</li>
</ul>
<h2 id="references"><strong>References</strong></h2>
<p>Arian et.al.,<U><a href="https://arxiv.org/abs/2105.09121" target="_blank"> Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead </a></U>, arXiv, 2021.<br>
Joshua et.al.,<U><a href="https://arxiv.org/abs/2303.09752" target="_blank"> COLT5: Faster Long-Range Transformers with Conditional Computation </a></U>, EMNLP, 2023.<br>
Noam et.al.,<U><a href="https://arxiv.org/abs/1701.06538" target="_blank"> OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER </a></U>, ICLR, 2017.
AstraMind AI (2024). Unofficial implementation for the paper &ldquo;Mixture-of-Depths&rdquo;. <a href="https://github.com/astramind-ai/Mixture-of-depths" target="_blank" rel="noopener">https://github.com/astramind-ai/Mixture-of-depths</a>.</p>

      </div>

      
  <time class="mt-12 mb-8 block text-xs text-gray-500 ltr:text-right rtl:text-left dark:text-gray-400" datetime="2023-10-26T00:00:00.000Z">
    <span>Last updated on</span>
    Oct 26, 2023</time>

      <div class="container mx-auto prose prose-slate lg:prose-xl dark:prose-invert mt-5">
        
        <div class="max-w-prose print:hidden">
  
  

  

<div class="flex justify-center">
  
  <a class="no-underline bg-primary-100 hover:bg-primary-300 text-primary-800 text-xs font-medium mr-2 px-2.5 py-0.5 lg:px-5 lg:py-2 rounded dark:bg-primary-900 dark:hover:bg-primary-700 dark:text-primary-300" href="/tags/academic/">Academic</a>
  
</div>


  
<section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="https://twitter.com/intent/tweet?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fmixture-of-depth%2F&amp;text=%F0%9F%A4%96&#43;Accelerating&#43;Transformers&#43;via&#43;Conditional&#43;Computation&#43;-&#43;As&#43;Aspect&#43;of&#43;Mixture-of-Depths"
    title="X"
    aria-label="X"
    id="share-link-x"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </a>
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fmixture-of-depth%2F&amp;t=%F0%9F%A4%96&#43;Accelerating&#43;Transformers&#43;via&#43;Conditional&#43;Computation&#43;-&#43;As&#43;Aspect&#43;of&#43;Mixture-of-Depths"
    title="Facebook"
    aria-label="Facebook"
    id="share-link-facebook"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="currentColor" d="M22 12c0-5.52-4.48-10-10-10S2 6.48 2 12c0 4.84 3.44 8.87 8 9.8V15H8v-3h2V9.5C10 7.57 11.57 6 13.5 6H16v3h-2c-.55 0-1 .45-1 1v2h3v3h-3v6.95c5.05-.5 9-4.76 9-9.95z"/></svg>
  </a>
  

  
  
  
  
  
  
    
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="mailto:?subject=%F0%9F%A4%96%20Accelerating%20Transformers%20via%20Conditional%20Computation%20-%20As%20Aspect%20of%20Mixture-of-Depths&amp;body=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fmixture-of-depth%2F"
    title="Email"
    aria-label="Email"
    id="share-link-email"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5 0 1 1-9 0a4.5 4.5 0 0 1 9 0Zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 1 0-2.636 6.364M16.5 12V8.25"/></svg>
  </a>
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="https://www.linkedin.com/shareArticle?url=http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fmixture-of-depth%2F&amp;title=%F0%9F%A4%96&#43;Accelerating&#43;Transformers&#43;via&#43;Conditional&#43;Computation&#43;-&#43;As&#43;Aspect&#43;of&#43;Mixture-of-Depths"
    title="LinkedIn"
    aria-label="LinkedIn"
    id="share-link-linkedin"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
  </a>
  

  
  
  
  
  
  
  <a
    target="_blank" rel="noopener"
    class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
    href="whatsapp://send?text=%F0%9F%A4%96&#43;Accelerating&#43;Transformers&#43;via&#43;Conditional&#43;Computation&#43;-&#43;As&#43;Aspect&#43;of&#43;Mixture-of-Depths%20http%3A%2F%2Flocalhost%3A1313%2Fpost%2Fmixture-of-depth%2F"
    title="WhatsApp"
    aria-label="WhatsApp"
    id="share-link-whatsapp"
  >
    <svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256" fill="currentColor"><path d="m187.58 144.84-32-16a8 8 0 0 0-8 .5l-14.69 9.8a40.55 40.55 0 0 1-16-16l9.8-14.69a8 8 0 0 0 .5-8l-16-32A8 8 0 0 0 104 64a40 40 0 0 0-40 40 88.1 88.1 0 0 0 88 88 40 40 0 0 0 40-40 8 8 0 0 0-4.42-7.16ZM152 176a72.08 72.08 0 0 1-72-72 24 24 0 0 1 19.29-23.54l11.48 23L101 118a8 8 0 0 0-.73 7.51 56.47 56.47 0 0 0 30.15 30.15A8 8 0 0 0 138 155l14.61-9.74 23 11.48A24 24 0 0 1 152 176ZM128 24a104 104 0 0 0-91.82 152.88l-11.35 34.05a16 16 0 0 0 20.24 20.24l34.05-11.35A104 104 0 1 0 128 24Zm0 192a87.87 87.87 0 0 1-44.06-11.81 8 8 0 0 0-6.54-.67L40 216l12.47-37.4a8 8 0 0 0-.66-6.54A88 88 0 1 1 128 216Z"/></svg>
  </a>
  
</section>


  








  
  
    



  
  
  
    
  
  
  

<div class="flex pt-12 pb-4">
  
  
  <img
    class="mr-4 h-24 w-24 rounded-full"
    width="96"
    height="96"
    alt="Inkwan Hwang"
  src="/author/inkwan-hwang/avatar_hu15603882764808517387.jpg"
  loading="lazy"
  />
  
  <div class="place-self-center">
    <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
      Authors
    </div>
    <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
      <a href="http://localhost:1313/" class="no-underline">
      Inkwan Hwang
      </a>
    </div>

    
    <div class="text-sm font-bold text-neutral-700 dark:text-neutral-300">
    Senior Student
    </div>
    


    

    <div class="text-2xl sm:text-lg pt-1">

      
<div class="flex flex-wrap text-neutral-500 dark:text-neutral-300">
  
    
    
    
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="mailto:ikhwang@postech.ac.kr"
      
      aria-label="At-Symbol"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5 0 1 1-9 0a4.5 4.5 0 0 1 9 0Zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 1 0-2.636 6.364M16.5 12V8.25"/></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://x.com/inkwan_hwang"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Brands/X"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://github.com/inkwanhwang"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Brands/Github"
    ><svg style="height: 1em;" fill="currentColor" viewBox="3 3 18 18"><path d="M12 3C7.0275 3 3 7.12937 3 12.2276C3 16.3109 5.57625 19.7597 9.15374 20.9824C9.60374 21.0631 9.77249 20.7863 9.77249 20.5441C9.77249 20.3249 9.76125 19.5982 9.76125 18.8254C7.5 19.2522 6.915 18.2602 6.735 17.7412C6.63375 17.4759 6.19499 16.6569 5.8125 16.4378C5.4975 16.2647 5.0475 15.838 5.80124 15.8264C6.51 15.8149 7.01625 16.4954 7.18499 16.7723C7.99499 18.1679 9.28875 17.7758 9.80625 17.5335C9.885 16.9337 10.1212 16.53 10.38 16.2993C8.3775 16.0687 6.285 15.2728 6.285 11.7432C6.285 10.7397 6.63375 9.9092 7.20749 9.26326C7.1175 9.03257 6.8025 8.08674 7.2975 6.81794C7.2975 6.81794 8.05125 6.57571 9.77249 7.76377C10.4925 7.55615 11.2575 7.45234 12.0225 7.45234C12.7875 7.45234 13.5525 7.55615 14.2725 7.76377C15.9937 6.56418 16.7475 6.81794 16.7475 6.81794C17.2424 8.08674 16.9275 9.03257 16.8375 9.26326C17.4113 9.9092 17.76 10.7281 17.76 11.7432C17.76 15.2843 15.6563 16.0687 13.6537 16.2993C13.98 16.5877 14.2613 17.1414 14.2613 18.0065C14.2613 19.2407 14.25 20.2326 14.25 20.5441C14.25 20.7863 14.4188 21.0746 14.8688 20.9824C16.6554 20.364 18.2079 19.1866 19.3078 17.6162C20.4077 16.0457 20.9995 14.1611 21 12.2276C21 7.12937 16.9725 3 12 3Z"></path></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="https://www.linkedin.com/in/ikhwang"
      target="_blank" rel="noopener" rel="me noopener noreferrer"
      aria-label="Brands/Linkedin"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a>
  
    
    
    
    
      
    
    <a
      class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400"
      style="will-change:transform;"
      href="/"
      
      aria-label="Academicons/Cv"
    ><svg style="height: 1em;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M249.18 328.324c-9.788 15.384-19.179 30.434-40.222 45.055c-11.256 7.89-37.164 23.306-73.99 23.306C64.709 396.685 8 345.605 8 255.801c0-78.486 53.345-140.486 128.466-140.486c30.434 0 57.474 10.521 77.387 26.304c18.414 14.65 27.038 29.304 34.563 42.456l-52.58 26.273c-3.762-8.626-8.29-17.649-19.913-27.406c-12.784-10.155-25.54-13.152-36.46-13.152c-42.821 0-65.364 39.825-65.364 84.145c0 58.238 29.7 87.143 65.364 87.143c34.563 0 48.48-24.042 57.474-39.426l52.243 26.673zm184.194-204.75H504l-92.037 265.22h-67.597l-90.904-265.22h70.625l54.843 188.6z"/></svg></a>
  
</div>



    </div>
  </div>
</div>



  
    




  




  
  
    
    
    
      
      
    
<div class="pt-1 no-prose w-full">
  <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
  <div class="flex flex-col md:flex-row flex-nowrap justify-between gap-5 pt-2">
    <div class="">
      
    </div>
    <div class="">
      
        <a class="group flex text-right no-underline" href="/post/summer-break/">
          <span class="flex flex-col">
            <span
              class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
            >üèñÔ∏è Summer Break - Chance to See the Wider World</span
            >
            <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
              
                Sep 14, 2022
              
            </span>
          </span>
          <span
            class="mt-[-0.3rem] ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
          ><span class="ltr:inline">&rarr;</span></span>
        </a>
      
    </div>
  </div>
</div>



  


  



</div>

      </div>

    </main>
  </article>
</div>

    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  <p class="powered-by text-center">
    ¬© 2024 Inkwan Hwang.
  </p>
  





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> ‚Äî the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
